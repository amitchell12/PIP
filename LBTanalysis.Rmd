---
title: "LBT EWS Analysis"
author: "A.G. Mitchell"
date: "2022-10-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reshape2)
```

# Load data
```{r}
aPath <- '/Users/au706616/Documents/Experiments/PIPTOT/analysis/'
fileName <- 'LBT_compiled-data.csv'
all_dat <- read.csv(file.path(aPath,fileName)) 
```

```{r}
## First, code bisection error
#all_dat$bisect_x_scaled <- all_dat$bisect_x/all_dat$av_rate
# before calculating bisection error check bisect_x and calib_loc_mid make sense
#bisect_check <- aggregate(bisect_x_scaled ~ prolific_id*left_mm*right_mm*calib_loc_midy_x, 
 #                         mean, data = all_dat)
# yep that seems to make sense, now convert and do the same
all_dat$bisect_error <- all_dat$bisect_x - all_dat$calib_loc_midy_x
# then scale the error by coordinate change rate calculated in previous step
all_dat$bisect_error_scaled <- all_dat$bisect_error/all_dat$av_rate
all_dat$bisect_error_scaled <- all_dat$bisect_error_scaled/all_dat$pix_permm

bisect_check <- aggregate(bisect_error_scaled ~ prolific_id*left_mm*right_mm, 
                          mean, data = all_dat)
bisect_check$offset <- (bisect_check$left_mm + bisect_check$right_mm)/2
# yes this step makes sense for most people, plot it just incase

ggplot(bisect_check, aes(group = as.factor(offset), colour = as.factor(offset))) +
  geom_point(aes(bisect_error_scaled, y = 0)) +
  xlim(-75,75) +
  theme_bw() +
  facet_wrap(~prolific_id)


# some more filtering
# remove trials with response > 3000ms
all_dat <- dplyr::filter(all_dat, response_time < 3000)

# identify any participants with too few trials, less than 32 per condition
trials <- aggregate(bisect_error ~ prolific_id*pip*block, length, data = all_dat)
id_x <- trials$prolific_id[trials$bisect_error < 32]

# check all participants have levels of both factors (block and tone)
nblock <- aggregate(bisect_error ~ block*prolific_id, length, data = all_dat)
nblock <- nblock %>% 
  group_by(prolific_id) %>%
  tally()

ntone <- aggregate(bisect_error ~ pip*prolific_id, length, data = all_dat)
ntone <- ntone %>% 
  group_by(prolific_id) %>%
  tally()

# and remove those that don't
id_x <- append(id_x, nblock$prolific_id[nblock$n < 2])
id_x <- append(id_x, ntone$prolific_id[nblock$n < 2])

all_dat <- filter(all_dat, !(prolific_id %in% id_x))
n <- count(all_dat, 'prolific_id')
length(n$prolific_id)

# recode ID
all_dat$prolific_id <- as.factor(all_dat$prolific_id)
all_dat$ID <- as.character(as.numeric(all_dat$prolific_id))
all_dat <- all_dat[order(all_dat$ID), ]
rownames(all_dat) <- NULL
```

# ANALYSIS
End point weightings on both block and tone conditions
```{r}
# recode bisection variables
all_dat$P <- all_dat$bisect_error_scaled
all_dat$L <- all_dat$left_mm
all_dat$R <- all_dat$right_mm
all_dat$ID <- as.factor(all_dat$ID)

# tone and block recoding
all_dat$cond[all_dat$block == 1 & all_dat$pip == 0] <- '1' #block early, tone blank
all_dat$cond[all_dat$block == 1 & all_dat$pip == 1] <- '2' #block early, tone sound
all_dat$cond[all_dat$block == 2 & all_dat$pip == 0] <- '3' #block late, tone blank
all_dat$cond[all_dat$block == 2 & all_dat$pip == 1] <- '4' #block late, tone sound

all_dat$block <- ifelse(all_dat$block == 1, 'EARLY', 'LATE')
all_dat$tone <- ifelse(all_dat$pip == 0, 'BLANK', 'SOUND')

# as factors
all_dat$cond <- as.factor(all_dat$cond)
all_dat$block <- as.factor(all_dat$block)
all_dat$tone <- as.factor(all_dat$tone)

# End point weightings analysis
# TONE
EW_DV <- read.csv(text="ID,tone,block,cond,NUMTRIALS,rsq,k,dPL,dPR")

for(ID in levels(all_dat$ID)){
  for(cond in levels(all_dat$cond)){
    tmp <- all_dat[all_dat$ID == ID & all_dat$cond==cond, 
                   c("bisect_error_scaled","pix_permm","P","L","R")]
    model <- lm(P~L+R, data=tmp)
    #tone = tmp$tone
    #block = tmp$block
    NUMTRIALS <- nrow(tmp)
    rsq <- summary(model)$r.squared
    k <- as.numeric(coefficients(model)[1])
    dPL <- as.numeric(coefficients(model)[2])
    dPR <- as.numeric(coefficients(model)[3])
    
    #add to dataframe
    EW_DV <- rbind(EW_DV, cbind.data.frame(ID,cond,NUMTRIALS,rsq,k,dPL,dPR))
  }
}

#calculate composites
EW_DV$EWB <- EW_DV$dPR-EW_DV$dPL
EW_DV$EWS <- EW_DV$dPR+EW_DV$dPL

EW_DV <- EW_DV %>% 
  rowwise() %>%
  mutate(
    FILT = case_when(rsq < .7 ~ TRUE,
               EWB > .5 ~ TRUE,
               EWS < .5 ~ TRUE
           ))

# flag ids for removal
id_filt <- EW_DV$ID[EW_DV$FILT == TRUE]
id_filt <- id_filt[!is.na(id_filt)]


# calculate final n of participants that can be used (this will be low)
all_dat <- filter(all_dat, !(ID %in% id_filt))
EW_DV <- filter(EW_DV, !(ID %in% id_filt))
final_n <- count(all_dat, 'ID')
length(final_n$ID)
```

# Reliability and distribution checks on data
```{r}
# plot EWS distribution
ggplot(EW_DV, aes(EWB)) +
  geom_density() +
  theme_bw()

ggplot(EW_DV, aes(EWS)) +
  geom_density() +
  theme_bw()

# plot all dPL and dPR to see why EWS is so high
ggplot(EW_DV, aes(dPL, dPR)) +
  geom_point() +
  geom_hline(yintercept = .5) +
  geom_vline(xintercept = .5) +
  ylim(0.3,0.7) + 
  xlim(0.3,0.7) +
  theme_bw()

# test re-test
# cast by block
EW_DV$block <- ifelse(EW_DV$cond == '1' | EW_DV$cond == '2', 'EARLY', 'LATE')

block1 <- filter(EW_DV, block == 'EARLY')
names(block1)[8] <- 'EWB_1'
names(block1)[9] <- 'EWS_1'
block2 <- filter(EW_DV, block == 'LATE')
names(block2)[8] <- 'EWB_2'
names(block2)[9] <- 'EWS_2'

DF <- merge(block1, block2, by = 'ID')

# run test re-test

cor(DF$EWS_1, DF$EWS_2)
cor(DF$EWB_1, DF$EWB_2)
```

# Analysis across blocks
# Analysis across tones

# Directional bisection error

# Test for pseudoneglect

```{r}
```

# Further checks

```{r}
# time of breaks
bisectData$break_length <- bisectData$time_break_time_stamp - bisectData$time_block_one_end 
bisectData$break_length <- bisectData$break_length/100 #in seconds
```
